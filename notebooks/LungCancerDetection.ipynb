{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I am using LUNA16 competition dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lung cancer is the leading cause of cancer-related death worldwide. Screening high risk individuals for lung cancer with low-dose CT scans is now being implemented in the United States and other countries are expected to follow soon. In CT lung cancer screening, many millions of CT scans will have to be analyzed, which is an enormous burden for radiologists. Therefore there is a lot of interest to develop computer algorithms to optimize screening.?\n",
    "\n",
    "The upcoming high-profile?Coding4Cancer?challenge invites coders to create the best computer algorithm that can identify a person as having lung cancer based on one or multiple low-dose CT images.\n",
    "\n",
    "To be able to solve the Coding4Cancer challenge, and detect lung cancer in an early stage, pulmonary nodules, the early manifestation of lung cancers, have to be located. Many Computer-aided detection (CAD) systems have already been proposed for this task. The LUNA16 challenge will focus on a large-scale evaluation of automatic nodule detection algorithms on the publicly available LIDC/IDRI dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to do!\n",
    "### Monday\n",
    "1. .mhd files (extract 3D image data) - done\n",
    "\n",
    "### Tuesday\n",
    "2. Extract 2D image slice based on the coordinates?\n",
    "3. Preprocess data\n",
    "\n",
    "\n",
    "### Day after + Thursday\n",
    "4. Train a CNN\n",
    "5. Validate using their evaluation\n",
    "6. Uncertainty quantification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import SimpleITK as sitk\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from scipy.misc import imread\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "annotations = pd.read_csv('../src/data/annotations.csv')\n",
    "candidates = pd.read_csv('../src/data/candidates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "candidates['class'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "len(annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidates have two classes, one with nodules, one without"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "candidates.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print len(candidates[candidates['class'] == 1])\n",
    "print len(candidates[candidates['class'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "print num_cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes are heaviliy unbalanced, hardly 0.2% percent are positive.\n",
    "\n",
    "The best way to move forward will be to undersample the negative class and then augment the positive class heaviliy to balance out the samples.\n",
    "\n",
    "#### Plan of attack:\n",
    "\n",
    "1. Get an initial subsample of negative class and keep all of the positives such that we have a 80/20 class distribution\n",
    "\n",
    "2. Create a training set such that we augment minority class heavilby rotating to get a 50/50 class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class CTScan(object):\n",
    "    def __init__(self, filename = None, coords = None):\n",
    "        self.filename = filename\n",
    "        self.coords = coords\n",
    "        self.ds = None\n",
    "        self.image = None\n",
    "\n",
    "    def reset_coords(self, coords):\n",
    "        self.coords = coords\n",
    "\n",
    "    def read_mhd_image(self):\n",
    "        path = glob.glob('../data/raw/*/'+ self.filename + '.mhd')\n",
    "        self.ds = sitk.ReadImage(path[0])\n",
    "        self.image = sitk.GetArrayFromImage(self.ds)\n",
    "\n",
    "    def get_resolution(self):\n",
    "        return self.ds.GetSpacing()\n",
    "\n",
    "    def get_origin(self):\n",
    "        return self.ds.GetOrigin()\n",
    "\n",
    "    def get_ds(self):\n",
    "        return self.ds\n",
    "\n",
    "    def get_voxel_coords(self):\n",
    "        origin = self.get_origin()\n",
    "        resolution = self.get_resolution()\n",
    "        voxel_coords = [np.absolute(self.coords[j]-origin[j])/resolution[j] \\\n",
    "            for j in range(len(self.coords))]\n",
    "        return tuple(voxel_coords)\n",
    "    \n",
    "    def get_image(self):\n",
    "        return self.image\n",
    "    \n",
    "    def get_subimage(self, width):\n",
    "        self.read_mhd_image()\n",
    "        x, y, z = self.get_voxel_coords()\n",
    "        subImage = self.image[z, y-width/2:y+width/2, x-width/2:x+width/2]\n",
    "        return subImage   \n",
    "    \n",
    "    def normalizePlanes(self, npzarray):\n",
    "        maxHU = 400.\n",
    "        minHU = -1000.\n",
    "        npzarray = (npzarray - minHU) / (maxHU - minHU)\n",
    "        npzarray[npzarray>1] = 1.\n",
    "        npzarray[npzarray<0] = 0.\n",
    "        return npzarray\n",
    "    \n",
    "    def save_image(self, filename, width):\n",
    "        image = self.get_subimage(width)\n",
    "        image = self.normalizePlanes(image)\n",
    "        Image.fromarray(image*255).convert('L').save(filename)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = candidates[candidates['class']==1].index  \n",
    "negatives = candidates[candidates['class']==0].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Check if my class works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "scan = CTScan(np.asarray(candidates.iloc[negatives[600]])[0], \\\n",
    "              np.asarray(candidates.iloc[negatives[600]])[1:-1])\n",
    "scan.read_mhd_image()\n",
    "x, y, z = scan.get_voxel_coords()\n",
    "image = scan.get_image()\n",
    "dx, dy, dz = scan.get_resolution()\n",
    "x0, y0, z0 = scan.get_origin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it on a test set you know works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "filename = '1.3.6.1.4.1.14519.5.2.1.6279.6001.100398138793540579077826395208'\n",
    "coords = (70.19, -140.93, 877.68)#[877.68, -140.93, 70.19]\n",
    "scan = CTScan(filename, coords)\n",
    "scan.read_mhd_image()\n",
    "x, y, z = scan.get_voxel_coords()\n",
    "image = scan.get_image()\n",
    "dx, dy, dz = scan.get_resolution()\n",
    "x0, y0, z0 = scan.get_origin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Ok the class to get image data works\n",
    "\n",
    "Next thing to do is to undersample negative class drastically. Since the number of positives in the data set of 551065 are 1351 and rest are negatives, I plan to make the dataset less skewed. Like a 70%/30% split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "negIndexes = np.random.choice(negatives, len(positives)*5, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "candidatesDf = candidates.iloc[list(positives)+list(negIndexes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now split it into test train set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X = candidatesDf.iloc[:,:-1]\n",
    "y = candidatesDf.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_pickle('traindata')\n",
    "X_test.to_pickle('testdata')\n",
    "X_val.to_pickle('valdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizePlanes(npzarray):\n",
    "    maxHU = 400.\n",
    "    minHU = -1000.\n",
    "    npzarray = (npzarray - minHU) / (maxHU - minHU)\n",
    "    npzarray[npzarray>1] = 1.\n",
    "    npzarray[npzarray<0] = 0.\n",
    "    return npzarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focus on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print 'number of positive cases are ' + str(y_train.sum())\n",
    "print 'total set size is ' + str(len(y_train))\n",
    "print 'percentage of positive cases are ' + str(y_train.sum()*1.0/len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are 845 positive cases out of 5187 cases in the training set. We will need to augment the positive dataset like mad. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new keys to X_train and y_train for augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tempDf = X_train[y_train == 1]\n",
    "tempDf = tempDf.set_index(X_train[y_train == 1].index + 1000000)\n",
    "X_train_new = X_train.append(tempDf)\n",
    "tempDf = tempDf.set_index(X_train[y_train == 1].index + 2000000)\n",
    "X_train_new = X_train_new.append(tempDf)\n",
    "\n",
    "ytemp = y_train.reindex(X_train[y_train == 1].index + 1000000)\n",
    "ytemp.loc[:] = 1\n",
    "y_train_new = y_train.append(ytemp)\n",
    "ytemp = y_train.reindex(X_train[y_train == 1].index + 2000000)\n",
    "ytemp.loc[:] = 1\n",
    "y_train_new = y_train_new.append(ytemp)\n",
    "\n",
    "print len(X_train_new), len(y_train_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X_train_new.index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.misc import imresize\n",
    "from PIL import ImageEnhance\n",
    "\n",
    "class PreProcessing(object):\n",
    "    def __init__(self, image = None):\n",
    "        self.image = image\n",
    "    \n",
    "    def subtract_mean(self):\n",
    "        self.image = (self.image/255.0 - 0.25)*255\n",
    "        return self.image\n",
    "    \n",
    "    def downsample_data(self):\n",
    "        self.image = imresize(self.image, size = (40, 40), interp='bilinear', mode='L')\n",
    "        return self.image\n",
    "    \n",
    "    def enhance_contrast(self):\n",
    "        self.image = ImageEnhance.Contrast(self.image)\n",
    "        return self.image\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dirName = '../src/data/train/'\n",
    "plt.figure(figsize = (10,10))\n",
    "inp = imread(dirName + 'image_'+ str(30517) + '.jpg')\n",
    "plt.subplot(221)\n",
    "plt.imshow(inp)\n",
    "plt.grid(False)\n",
    "\n",
    "Pp = PreProcessing(inp)\n",
    "\n",
    "inp2 = Pp.subtract_mean()\n",
    "plt.subplot(222)\n",
    "plt.imshow(inp2)\n",
    "plt.grid(False)\n",
    "\n",
    "#inp4 = Pp.enhance_contrast()\n",
    "#plt.subplot(224)\n",
    "#plt.imshow(inp4)\n",
    "#plt.grid(False)\n",
    "\n",
    "inp3 = Pp.downsample_data()\n",
    "plt.subplot(223)\n",
    "plt.imshow(inp3)\n",
    "plt.grid(False)\n",
    "\n",
    "#inp4 = Pp.enhance_contrast()\n",
    "#plt.subplot(224)\n",
    "#plt.imshow(inp4)\n",
    "#plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dirName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convnet stuff\n",
    "\n",
    "I am planning t  us tflearn which is a wrapper around tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import tflearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading image data on the fly is inefficient. So I am us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y_train_new.values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_filenames =\\\n",
    "X_train_new.index.to_series().apply(lambda x:\\\n",
    "                                    '../src/data/train/image_'+str(x)+'.jpg')\n",
    "train_filenames.values.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset_file = 'traindatalabels.txt'\n",
    "\n",
    "train_filenames =\\\n",
    "X_train_new.index.to_series().apply(lambda x:\\\n",
    "   \n",
    "filenames = train_filenames.values.astype(str)\n",
    "labels = y_train_new.values.astype(int)\n",
    "traindata = np.zeros(filenames.size,\\\n",
    "                     dtype=[('var1', 'S36'), ('var2', int)])\n",
    "traindata['var1'] = filenames\n",
    "traindata['var2'] = labels\n",
    "\n",
    "np.savetxt(dataset_file, traindata, fmt=\"%10s %d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Build a HDF5 dataset (only required once)\n",
    "from tflearn.data_utils import build_hdf5_image_dataset\n",
    "build_hdf5_image_dataset(dataset_file, image_shape=(50, 50), mode='file', output_path='traindataset.h5', categorical_labels=True, normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HDF5 dataset\n",
    "import h5py\n",
    "h5f = h5py.File('traindataset.h5', 'r')\n",
    "X_train_images = h5f['X']\n",
    "Y_train_labels = h5f['Y']\n",
    "\n",
    "h5f2 = h5py.File('../src/data/valdataset.h5', 'r')\n",
    "X_val_images = h5f2['X']\n",
    "Y_val_labels = h5f2['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading tflearn packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_preprocessing import ImagePreprocessing\n",
    "from tflearn.data_augmentation import ImageAugmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the data is normalized\n",
    "img_prep = ImagePreprocessing()\n",
    "img_prep.add_featurewise_zero_center()\n",
    "img_prep.add_featurewise_stdnorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create extra synthetic training data by flipping, rotating and blurring the\n",
    "# images on our data set.\n",
    "img_aug = ImageAugmentation()\n",
    "img_aug.add_random_flip_leftright()\n",
    "img_aug.add_random_rotation(max_angle=25.)\n",
    "img_aug.add_random_blur(sigma_max=3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input is a 50x50 image with 1 color channels (grayscale)\n",
    "network = input_data(shape=[None, 50, 50, 1],\n",
    "                     data_preprocessing=img_prep,\n",
    "                     data_augmentation=img_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convolution\n",
    "network = conv_2d(network, 50, 3, activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Max pooling\n",
    "network = max_pool_2d(network, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Convolution again\n",
    "network = conv_2d(network, 64, 3, activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Convolution yet again\n",
    "network = conv_2d(network, 64, 3, activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Max pooling again\n",
    "network = max_pool_2d(network, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Fully-connected 512 node neural network\n",
    "network = fully_connected(network, 512, activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Dropout - throw away some data randomly during training to prevent over-fitting\n",
    "network = dropout(network, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Fully-connected neural network with two outputs (0=isn't a nodule, 1=is a nodule) to make the final prediction\n",
    "network = fully_connected(network, 2, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell tflearn how we want to train the network\n",
    "network = regression(network, optimizer='adam',\n",
    "                     loss='categorical_crossentropy',\n",
    "                     learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Wrap the network in a model object\n",
    "model = tflearn.DNN(network, tensorboard_verbose=0, checkpoint_path='nodule-classifier.tfl.ckpt')\n",
    "\n",
    "# Train it! We'll do 100 training passes and monitor it as it goes.\n",
    "model.fit(X_train_images, Y_train_labels, n_epoch=100, shuffle=True, validation_set=(X_val_images, Y_val_labels),\n",
    "          show_metric=True, batch_size=96,\n",
    "          snapshot_epoch=True,\n",
    "          run_id='nodule-classifier')\n",
    "\n",
    "# Save model when training is complete to a file\n",
    "model.save(\"nodule-classifier.tfl\")\n",
    "print(\"Network trained and saved as nodule-classifier.tfl!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "h5f2 = h5py.File('../src/data/testdataset.h5', 'r')\n",
    "X_test_images = h5f2['X']\n",
    "Y_test_labels = h5f2['Y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model.predict(X_test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lungcancer",
   "language": "python",
   "name": "lungcancer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
